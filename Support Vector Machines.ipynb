{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, ConfusionMatrixDisplay\n",
    "import pickle\n",
    "from sklearn.multiclass import OneVsRestClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = []\n",
    "data= pd.read_csv('data_file.txt')\n",
    "y = data['Solution']\n",
    "X = data[features]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=10,stratify=y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 15.2 ms, sys: 3 Âµs, total: 15.2 ms\n",
      "Wall time: 13.7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "sc = StandardScaler()\n",
    "X_train = sc.fit_transform(X_train)\n",
    "X_test = sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier = SVC(kernel = 'rbf', probability = True, random_state = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifier1 = classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_file= 'save_path_SVM.sav'\n",
    "pickle.dump(classifier1, open(save_file,'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background' 'Cal. Soln' 'Conf. Check' 'EH' 'Wet_BG' 'YG. Soln']\n",
      "Confusion Matrix :\n",
      "[[40851    83   146  4696   396    56]\n",
      " [  115   351     0     0     0     0]\n",
      " [  434     0  1362     0     0     2]\n",
      " [ 3264     0     0 16863     3     0]\n",
      " [  922     0     0   542  2110     0]\n",
      " [   59     0     2     0     0   139]]\n",
      "Accuracy Score : 0.8519255207470026\n",
      "Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Background       0.89      0.88      0.89     46228\n",
      "   Cal. Soln       0.81      0.75      0.78       466\n",
      " Conf. Check       0.90      0.76      0.82      1798\n",
      "          EH       0.76      0.84      0.80     20130\n",
      "      Wet_BG       0.84      0.59      0.69      3574\n",
      "    YG. Soln       0.71      0.69      0.70       200\n",
      "\n",
      "    accuracy                           0.85     72396\n",
      "   macro avg       0.82      0.75      0.78     72396\n",
      "weighted avg       0.85      0.85      0.85     72396\n",
      "\n",
      "[[88.37, 0.18, 0.32, 10.16, 0.86, 0.12], [24.68, 75.32, 0.0, 0.0, 0.0, 0.0], [24.14, 0.0, 75.75, 0.0, 0.0, 0.11], [16.21, 0.0, 0.0, 83.77, 0.01, 0.0], [25.8, 0.0, 0.0, 15.17, 59.04, 0.0], [29.5, 0.0, 1.0, 0.0, 0.0, 69.5]]\n",
      "CPU times: user 3min 37s, sys: 0 ns, total: 3min 37s\n",
      "Wall time: 3min 37s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted= classifier1.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predicted, labels=classifier1.classes_)\n",
    "print(classifier1.classes_)\n",
    "print('Confusion Matrix :')\n",
    "print(cm)\n",
    "print('Accuracy Score :',accuracy_score(y_test,predicted))\n",
    "print('Report :')\n",
    "print(classification_report(y_test, predicted))\n",
    "pcts = []\n",
    "for i in range(len(cm)):\n",
    "    total = sum(cm[i])\n",
    "    pcts.append([])\n",
    "    for j in range(len(cm[i])):\n",
    "        val = cm[i][j]*100/total\n",
    "        pcts[i].append(round(val,2))\n",
    "print(pcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Background' 'Cal' 'Conf. Check' 'Wet_BG' 'YG']\n",
      "Confusion Matrix :\n",
      "[[12100   108   139   597    78]\n",
      " [  106   360     0     0     0]\n",
      " [  323     0  1473     0     2]\n",
      " [  634     0     0  2940     0]\n",
      " [   54     0     0     0   146]]\n",
      "Accuracy Score : 0.8929171038824764\n",
      "Report :\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "  Background       0.92      0.93      0.92     13022\n",
      "         Cal       0.77      0.77      0.77       466\n",
      " Conf. Check       0.91      0.82      0.86      1798\n",
      "      Wet_BG       0.83      0.82      0.83      3574\n",
      "          YG       0.65      0.73      0.69       200\n",
      "\n",
      "    accuracy                           0.89     19060\n",
      "   macro avg       0.82      0.81      0.81     19060\n",
      "weighted avg       0.89      0.89      0.89     19060\n",
      "\n",
      "[[92.92, 0.83, 1.07, 4.58, 0.6], [22.75, 77.25, 0.0, 0.0, 0.0], [17.96, 0.0, 81.92, 0.0, 0.11], [17.74, 0.0, 0.0, 82.26, 0.0], [27.0, 0.0, 0.0, 0.0, 73.0]]\n",
      "CPU times: user 11.5 s, sys: 0 ns, total: 11.5 s\n",
      "Wall time: 11.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "predicted= classifier1.predict(X_test)\n",
    "cm = confusion_matrix(y_test, predicted, labels=classifier1.classes_)\n",
    "print(classifier1.classes_)\n",
    "print('Confusion Matrix :')\n",
    "print(cm)\n",
    "print('Accuracy Score :',accuracy_score(y_test,predicted))\n",
    "print('Report :')\n",
    "print(classification_report(y_test, predicted))\n",
    "pcts = []\n",
    "for i in range(len(cm)):\n",
    "    total = sum(cm[i])\n",
    "    pcts.append([])\n",
    "    for j in range(len(cm[i])):\n",
    "        val = cm[i][j]*100/total\n",
    "        pcts[i].append(round(val,2))\n",
    "print(pcts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# proability true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CM(file_name, kernal_type):\n",
    "    features = []\n",
    "    data= pd.read_csv(file_name)\n",
    "    y = data['Solution']\n",
    "    X = data[features]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=10,stratify=y)\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    classifier = SVC(kernel = kernal_type, probability = True, random_state = 0)\n",
    "    classifier1 = classifier.fit(X_train, y_train)\n",
    "    predicted= classifier1.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, predicted, labels=classifier1.classes_)\n",
    "    print(classifier1.classes_)\n",
    "    print('Confusion Matrix :')\n",
    "    print(cm)\n",
    "    print('Accuracy Score :',accuracy_score(y_test,predicted))\n",
    "    print('Report :')\n",
    "    print(classification_report(y_test, predicted))\n",
    "    pcts = []\n",
    "    for i in range(len(cm)):\n",
    "        total = sum(cm[i])\n",
    "        pcts.append([])\n",
    "        for j in range(len(cm[i])):\n",
    "            val = cm[i][j]*100/total\n",
    "            pcts[i].append(round(val,2))\n",
    "    print(pcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM('data_file.txt', 'rbf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Origial SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def CM(file_name, kernal_type):\n",
    "    features = []\n",
    "    data= pd.read_csv(file_name)\n",
    "    y = data['Solution']\n",
    "    X = data[features]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state=10,stratify=y)\n",
    "    sc = StandardScaler()\n",
    "    X_train = sc.fit_transform(X_train)\n",
    "    X_test = sc.transform(X_test)\n",
    "    classifier = SVC(kernel = kernal_type, random_state = 0)\n",
    "    classifier1 = classifier.fit(X_train, y_train)\n",
    "    predicted= classifier1.predict(X_test)\n",
    "    cm = confusion_matrix(y_test, predicted, labels=classifier1.classes_)\n",
    "    print(classifier1.classes_)\n",
    "    print('Confusion Matrix :')\n",
    "    print(cm)\n",
    "    print('Accuracy Score :',accuracy_score(y_test,predicted))\n",
    "    print('Report :')\n",
    "    print(classification_report(y_test, predicted))\n",
    "    pcts = []\n",
    "    for i in range(len(cm)):\n",
    "        total = sum(cm[i])\n",
    "        pcts.append([])\n",
    "        for j in range(len(cm[i])):\n",
    "            val = cm[i][j]*100/total\n",
    "            pcts[i].append(round(val,2))\n",
    "    print(pcts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CM('data_file.txt', 'rbf')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
